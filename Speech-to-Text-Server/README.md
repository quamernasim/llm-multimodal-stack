# Speech-To-Text Server

This project sets up an **asynchronous FastAPI server** for transcribing `.wav` audio files using OpenAI's Whisper model from Hugging Face Transformers.

## ğŸš€ Features
- **FastAPI-based** server for handling transcription requests
- **Fully asynchronous** using `asyncio` to prevent blocking
- **Supports only `.wav` files** for simplicity
- **Uses `uv` for package management** (faster than `pip`)

---

## ğŸ”§ Usage
### **Step 1: Set Up a Virtual Environment**
```sh
uv sync --frozen --no-cache
```

### **Step 2: Start the Server**
Run the following command to start the FastAPI server:
```sh
uv run main.py
```

### **Alternatively: Using Docker**
```sh
docker build -t {name} .
```


```sh
docker run -p {port}:8000 {name}
```

### **Test the API**
You can test using `curl`:
```sh
curl -X 'POST' \
  'http://localhost:8000/transcribe/' \
  -H 'accept: application/json' \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@your_audio.wav'
```
Or use FastAPIâ€™s built-in interactive UI at:
```
http://localhost:8000/docs
```

---

## ğŸ“œ API Endpoints
### **POST /transcribe/**
- **Description:** Accepts `.wav` files and returns the transcribed text.
- **Request Body:**
  - `file`: The `.wav` audio file.
- **Response:**
```json
{
  "transcription": "Transcribed text here"
}
```

---

## ğŸ› ï¸ How It Works
1. **Loads Whisper model** from Hugging Face (`openai/whisper-tiny`)
2. **Accepts `.wav` file uploads**
3. **Reads audio file asynchronously**
4. **Runs Whisper inference in a separate thread** using `asyncio.to_thread()`
5. **Returns transcription as JSON**

---

## ğŸ—ï¸ Project Structure
```
Speech-to-Text-Server/
.
â”œâ”€â”€ archive/                â† Archived legacy files
â”‚   â”œâ”€â”€ main_new_old.py
â”‚   â””â”€â”€ main_old.py
â”œâ”€â”€ config/                 â† Central configuration (e.g., constants, env settings)
â”‚   â””â”€â”€ constants.py
â”œâ”€â”€ core/                   â† Core business logic
â”‚   â”œâ”€â”€ model.py            â† Whisper model loading/inference
â”‚   â”œâ”€â”€ preprocessing.py    â† Audio preprocessing (resampling, mono, etc.)
â”‚   â””â”€â”€ transcriber.py      â† ASR pipeline and orchestration
â”œâ”€â”€ Dockerfile              â† Containerized deployment
â”œâ”€â”€ main.py                 â† FastAPI app entrypoint
â”œâ”€â”€ pyproject.toml          â† Dependencies and metadata (Poetry or PEP 621-based)
â”œâ”€â”€ README.md               â† Project documentation
â””â”€â”€ uv.lock                 â† Lock file for reproducible builds

```



---

## âš¡ Performance Optimization
- **Inference runs in a separate thread** to avoid blocking FastAPIâ€™s event loop
- **Uses `uv` for faster package management**
- **Preloads Whisper model** to reduce startup time

---

## ğŸ“¬ Contact
For issues or contributions, open a GitHub issue or contact [narendra@machinelearning.one].
