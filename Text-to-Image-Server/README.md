# Text-to-Image-Server

This project sets up an asynchronous FastAPI server for generating Images using the black-forest-labs/FLUX.1-schnell model. It is designed to handle text-to-image requests efficiently, returning generated images in response to user input.

## 🚀 Features
- **FastAPI-based** server for handling TTImg requests
- **Fully asynchronous** using asyncio to prevent blocking
- **Returns images** in response to text input
- **Uses `uv` for package management** (faster than pip)

---

## 🔧 Usage
### **Step 1: Set Up a Virtual Environment**
```sh
uv sync --frozen --no-cache
```

### **Step 2: Start the Server**
Run the following command to start the FastAPI server:
```sh
uv run main.py
```

### **Alternatively: Using Docker**
```sh
docker build -t {name} .
```


```sh
docker run -p {port}:8000 {name}
```

### **Test the API**
You can test using `curl`:
```sh
curl -X 'POST' \
  'http://localhost:8000/generate/' \
  -H 'accept: image/jgp' \
  -H 'Content-Type: application/json' \
  -d '{"text": "Come stai?"}' \
  --output output.jpg
```
Or use FastAPI’s built-in interactive UI at:
```
http://localhost:8000/docs
```

---

## 📜 API Endpoints
### **POST /generate/**
- **Description:** Accepts text input and returns a .jpg image generated by the FLUX.1-schnell model.
- **Request Body:**
  - `text`: The text to convert to an image.
- **Response**:
  Returns a `.jpg` image file.

---

## 🛠️ How It Works
1. **Loads the FLUX.1-schnell model** asynchronously
2. **Processes input text asynchronously**
3. **Returns the generated .jpg image** without blocking the server

---

## 🏗️ Project Structure
```
Text-to-Speech-Server/
│── Dockerfile        # For operationalizing 
│── README.md         # Project documentation
│── main.py           # FastAPI server with async synthesizing
│── .venv/            # Virtual environment (if sync is ran)
```

---

## ⚡ Performance Optimization
- **Inference runs in a separate thread** to avoid blocking FastAPI’s event loop
- **Uses `uv` for faster package management**
- **Preloads the model** to reduce latency on first request

---

## 📬 Contact
For issues or contributions, open a GitHub issue or contact [narendra@machinelearning.one].
